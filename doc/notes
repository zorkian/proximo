just doodling some config ideas for various setups...

# setup a master-master setup, this could technically have as many
# masters as you want.  with this setup, queries will only go to one
# machine from the list, if that machine goes down then we will start
# sending queries to a different machine and stop using the first one.
 
create mysql cluster mymastermaster
  setup = all-master
  masters = x:a, y:b, z:c
  behavior = one-at-a-time

# or you could say to split traffic among the databases...

  behavior = split

# but if you wanted to define a master/slave setup, you could do something like
# this... which drives traffic to your slaves, and writes to the master.  note
# that we are not including the master for reads.  which is something you might
# want to do if you have only a single slave, for example.

create mysql cluster mymasterslave
  setup = master-slave
  master = x:a
  slaves = y:b, z:c
  behavior = split
  include_master_for_reads = no

# what are some other setups?  a ring topology?  well, in that case it's
# kind of all masters...

=====================

and now talking about how to organize the clusters in the code... does it make
sense for the think receiving queries to be responsible for sending them out?  or
should there be a separate class that has smarts about what it's doing and can
do the proper thing?

so the way I'm thinking is, there's a P::M::Cluster module, that basically is
used as a way of defining what a cluster does.  then we have the same but as
P::M::Cluster::Instance which is an instantiation of the above and is used for
keeping state.

this Instance class is then responsible for allocating any Backends that it needs
to connect to.  it owns them then.  we don't do connection pooling at this point
so this is okay, when we're done we can just disconnect them.  (future improvement
is to do proper pooling.)

I -think- this is going to work out well... then we can have proper logic in the
Cluster module for doing all sorts of things like deciding where to send traffic
based on the state of what's going on.  this also allows the user to drop in all
sorts of replacements on where to send the queries, if they wanted.

for example, they could totally not use a mysql cluster backend and could instead
use a postgres backend and setup a plugin or something to do the right thing with
regards to rewriting queries and the like...

===================

keeping current database in sync and changing it around might be too much work to do?  might be useful to have backends be ip:port:database?

can't think of any applications that change database with any regularity...

need to start testing the other modes, see if we can get a 2nd or 3rd mysql up.. maybe connecting to some running in parallels that'd probably be easier

we start transactions on write, that's probably wrong
we might be able to get away with setting sticky, which expires in 5s
and using the result of the query to look at the flag to see if we're in a trans now
see how this affects BEGIN?
what about AUTO_COMMIT?

what about SET NAMES and stuff?

honestly all of this munging is just to make connection pooling work
but that might be a lost cause...
or maybe, make it so that the second it does something like SET, the connection gets 'saved' to that instance, and is always used there only
this lets things like bouncer use pooling
but bugzilla doesn't
this is a stopgap for now until more work is done to make it do the right stuff

remember the primary use case of bugzilla in NL